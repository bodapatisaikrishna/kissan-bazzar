{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2bfb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Expected columns ['chapter', 'verse', 'text', 'translation'], but found Index(['ID', 'Chapter', 'Verse', 'Shloka', 'Transliteration', 'HinMeaning',\n",
      "       'EngMeaning', 'WordMeaning'],\n",
      "      dtype='object')\n",
      "Dataset loaded successfully. Sample:\n",
      "      ID  Chapter  Verse                                             Shloka  \\\n",
      "0  BG1.1        1      1  धृतराष्ट्र उवाच |\\nधर्मक्षेत्रे कुरुक्षेत्रे स...   \n",
      "1  BG1.2        1      2  सञ्जय उवाच |\\nदृष्ट्वा तु पाण्डवानीकं व्यूढं द...   \n",
      "2  BG1.3        1      3  पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\\n...   \n",
      "3  BG1.4        1      4  अत्र शूरा महेष्वासा भीमार्जुनसमा युधि |\\nयुयुध...   \n",
      "4  BG1.5        1      5  धृष्टकेतुश्चेकितानः काशिराजश्च वीर्यवान् |\\nपु...   \n",
      "\n",
      "                                     Transliteration  \\\n",
      "0  dhṛtarāṣṭra uvāca .\\ndharmakṣetre kurukṣetre s...   \n",
      "1  sañjaya uvāca .\\ndṛṣṭvā tu pāṇḍavānīkaṃ vyūḍha...   \n",
      "2  paśyaitāṃ pāṇḍuputrāṇāmācārya mahatīṃ camūm .\\...   \n",
      "3  atra śūrā maheṣvāsā bhīmārjunasamā yudhi .\\nyu...   \n",
      "4  dhṛṣṭaketuścekitānaḥ kāśirājaśca vīryavān .\\np...   \n",
      "\n",
      "                                          HinMeaning  \\\n",
      "0  ।।1.1।।धृतराष्ट्र ने कहा -- हे संजय ! धर्मभूमि...   \n",
      "1  ।।1.2।।संजय ने कहा -- पाण्डव-सैन्य की व्यूह रच...   \n",
      "2  ।।1.3।।हे आचार्य ! आपके बुद्धिमान शिष्य द्रुपद...   \n",
      "3  ।।1.4।।इस सेना में महान् धनुर्धारी शूर योद्धा ...   \n",
      "4  ।।1.5।।धृष्टकेतु, चेकितान, बलवान काशिराज,  पुर...   \n",
      "\n",
      "                                          EngMeaning  \\\n",
      "0  1.1 Dhritarashtra said  What did my people and...   \n",
      "1  1.2. Sanjaya said  Having seen the army of the...   \n",
      "2  1.3. \"Behold, O Teacher! this mighty army of t...   \n",
      "3  1.4. Here are heroes, mighty archers, eal in b...   \n",
      "4  1.5. \"Dhrishtaketu, chekitana and the valiant ...   \n",
      "\n",
      "                                         WordMeaning context  \n",
      "0  1.1 धर्मक्षेत्रे on the holy plain? कुरुक्षेत्...          \n",
      "1  1.2 दृष्ट्वा having seen? तु indeed? पाण्डवानी...          \n",
      "2  1.3 पश्य behold? एताम् this? पाण्डुपुत्राणाम् ...          \n",
      "3  1.4 अत्र here? शूराः heroes? महेष्वासाः mighty...          \n",
      "4  1.5 धृष्टकेतुः Dhrishtaketu? चेकितानः Chekitan...          \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'chapter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chapter'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 135\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded successfully. Sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m--> 120\u001b[0m qa_data \u001b[38;5;241m=\u001b[39m create_qa_pairs(df)\n\u001b[1;32m    121\u001b[0m tokenized_dataset, tokenizer \u001b[38;5;241m=\u001b[39m prepare_hf_dataset(qa_data)\n\u001b[1;32m    122\u001b[0m model \u001b[38;5;241m=\u001b[39m train_model(tokenized_dataset)\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mcreate_qa_pairs\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     27\u001b[0m     context \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m     chapter \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchapter\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m     verse \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Generate synthetic QA pairs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chapter'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Step 1: Load and Preprocess the Dataset\n",
    "def load_dataset(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found at: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Assume columns: chapter, verse, text, translation (adjust based on your dataset)\n",
    "    expected_columns = ['chapter', 'verse', 'text', 'translation']\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        print(f\"Warning: Expected columns {expected_columns}, but found {df.columns}\")\n",
    "        # Combine available text columns for context\n",
    "        df['context'] = df.apply(lambda row: ' '.join([str(row[col]) for col in df.columns if col in ['text', 'translation', 'commentary']]), axis=1)\n",
    "    else:\n",
    "        df['context'] = df['text'] + \" \" + df['translation']\n",
    "    return df\n",
    "\n",
    "# Step 2: Format Data for Question-Answering\n",
    "def create_qa_pairs(df):\n",
    "    qa_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        context = row['context']\n",
    "        chapter = row['chapter']\n",
    "        verse = row['verse']\n",
    "        # Generate synthetic QA pairs\n",
    "        questions = [\n",
    "            f\"What does verse {verse} of chapter {chapter} say?\",\n",
    "            f\"What is the teaching in chapter {chapter} verse {verse}?\",\n",
    "            \"What is the meaning of this verse?\"\n",
    "        ]\n",
    "        for question in questions:\n",
    "            qa_data.append({\n",
    "                'question': question,\n",
    "                'context': context,\n",
    "                'answer': row.get('translation', context),  # Use translation or context as answer\n",
    "                'start_position': 0,  # Simplified; adjust for precise answer span\n",
    "                'end_position': len(row.get('translation', context))\n",
    "            })\n",
    "    return qa_data\n",
    "\n",
    "# Step 3: Prepare Dataset for Hugging Face\n",
    "def prepare_hf_dataset(qa_data):\n",
    "    dataset = Dataset.from_list(qa_data)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        encodings = tokenizer(\n",
    "            examples['question'],\n",
    "            examples['context'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        encodings['start_positions'] = examples['start_position']\n",
    "        encodings['end_positions'] = examples['end_position']\n",
    "        return encodings\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    return tokenized_dataset, tokenizer\n",
    "\n",
    "# Step 4: Fine-Tune the Model\n",
    "def train_model(tokenized_dataset):\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./gita_qa_model',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    model.save_pretrained('./gita_qa_model')\n",
    "    tokenizer.save_pretrained('./gita_qa_model')\n",
    "    return model\n",
    "\n",
    "# Step 5: Inference Function\n",
    "def answer_question(question, context, model, tokenizer):\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores) + 1\n",
    "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    return answer\n",
    "\n",
    "# Step 6: Main Function to Run the System\n",
    "def main():\n",
    "    # Path to your dataset\n",
    "    file_path = '/Users/bodapati/Downloads/Bhagwad_Gita.csv'\n",
    "    try:\n",
    "        df = load_dataset(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(\"Dataset loaded successfully. Sample:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    qa_data = create_qa_pairs(df)\n",
    "    tokenized_dataset, tokenizer = prepare_hf_dataset(qa_data)\n",
    "    model = train_model(tokenized_dataset)\n",
    "    \n",
    "    # Interactive QA loop\n",
    "    print(\"\\nBhagavad Gita QA System. Type 'exit' to quit.\")\n",
    "    context = \" \".join(df['context'].tolist())  # Combine all verses for general context\n",
    "    while True:\n",
    "        question = input(\"Ask a question about the Bhagavad Gita: \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        answer = answer_question(question, context, model, tokenizer)\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9c7c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['ID', 'Chapter', 'Verse', 'Shloka', 'Transliteration', 'HinMeaning', 'EngMeaning', 'WordMeaning']\n",
      "Dataset loaded successfully. Sample:\n",
      "      ID  Chapter  Verse                                             Shloka  \\\n",
      "0  BG1.1        1      1  धृतराष्ट्र उवाच |\\nधर्मक्षेत्रे कुरुक्षेत्रे स...   \n",
      "1  BG1.2        1      2  सञ्जय उवाच |\\nदृष्ट्वा तु पाण्डवानीकं व्यूढं द...   \n",
      "2  BG1.3        1      3  पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\\n...   \n",
      "3  BG1.4        1      4  अत्र शूरा महेष्वासा भीमार्जुनसमा युधि |\\nयुयुध...   \n",
      "4  BG1.5        1      5  धृष्टकेतुश्चेकितानः काशिराजश्च वीर्यवान् |\\nपु...   \n",
      "\n",
      "                                     Transliteration  \\\n",
      "0  dhṛtarāṣṭra uvāca .\\ndharmakṣetre kurukṣetre s...   \n",
      "1  sañjaya uvāca .\\ndṛṣṭvā tu pāṇḍavānīkaṃ vyūḍha...   \n",
      "2  paśyaitāṃ pāṇḍuputrāṇāmācārya mahatīṃ camūm .\\...   \n",
      "3  atra śūrā maheṣvāsā bhīmārjunasamā yudhi .\\nyu...   \n",
      "4  dhṛṣṭaketuścekitānaḥ kāśirājaśca vīryavān .\\np...   \n",
      "\n",
      "                                          HinMeaning  \\\n",
      "0  ।।1.1।।धृतराष्ट्र ने कहा -- हे संजय ! धर्मभूमि...   \n",
      "1  ।।1.2।।संजय ने कहा -- पाण्डव-सैन्य की व्यूह रच...   \n",
      "2  ।।1.3।।हे आचार्य ! आपके बुद्धिमान शिष्य द्रुपद...   \n",
      "3  ।।1.4।।इस सेना में महान् धनुर्धारी शूर योद्धा ...   \n",
      "4  ।।1.5।।धृष्टकेतु, चेकितान, बलवान काशिराज,  पुर...   \n",
      "\n",
      "                                          EngMeaning  \\\n",
      "0  1.1 Dhritarashtra said  What did my people and...   \n",
      "1  1.2. Sanjaya said  Having seen the army of the...   \n",
      "2  1.3. \"Behold, O Teacher! this mighty army of t...   \n",
      "3  1.4. Here are heroes, mighty archers, eal in b...   \n",
      "4  1.5. \"Dhrishtaketu, chekitana and the valiant ...   \n",
      "\n",
      "                                         WordMeaning  \\\n",
      "0  1.1 धर्मक्षेत्रे on the holy plain? कुरुक्षेत्...   \n",
      "1  1.2 दृष्ट्वा having seen? तु indeed? पाण्डवानी...   \n",
      "2  1.3 पश्य behold? एताम् this? पाण्डुपुत्राणाम् ...   \n",
      "3  1.4 अत्र here? शूराः heroes? महेष्वासाः mighty...   \n",
      "4  1.5 धृष्टकेतुः Dhrishtaketu? चेकितानः Chekitan...   \n",
      "\n",
      "                                             context  \n",
      "0  धृतराष्ट्र उवाच |\\nधर्मक्षेत्रे कुरुक्षेत्रे स...  \n",
      "1  सञ्जय उवाच |\\nदृष्ट्वा तु पाण्डवानीकं व्यूढं द...  \n",
      "2  पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् |\\n...  \n",
      "3  अत्र शूरा महेष्वासा भीमार्जुनसमा युधि |\\nयुयुध...  \n",
      "4  धृष्टकेतुश्चेकितानः काशिराजश्च वीर्यवान् |\\nपु...  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6e25cf96de4eb1b0e6f48df379718b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4869dc5bf5b744e488987c4bbb8f3346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928e0c9af2104c9291daf7b114508ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2709683d0c2747e59315def2a40fae0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d71013cd70548f188ed7b33662ef574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[6], line 134\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m    133\u001b[0m qa_data \u001b[38;5;241m=\u001b[39m create_qa_pairs(df)\n\u001b[0;32m--> 134\u001b[0m tokenized_dataset, tokenizer \u001b[38;5;241m=\u001b[39m prepare_hf_dataset(qa_data)\n\u001b[1;32m    135\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m train_model(tokenized_dataset)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Interactive QA loop\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 77\u001b[0m, in \u001b[0;36mprepare_hf_dataset\u001b[0;34m(qa_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m     encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_positions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_position\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encodings\n\u001b[0;32m---> 77\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_dataset, tokenizer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3525\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3524\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3525\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3526\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[1;32m   3527\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3475\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3475\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, apply_function(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3398\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3397\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3398\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[1;32m   3399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m, in \u001b[0;36mprepare_hf_dataset.<locals>.tokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m---> 65\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     66\u001b[0m         examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     67\u001b[0m         examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     68\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     69\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     70\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     71\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m     encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_positions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_position\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     74\u001b[0m     encodings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_positions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_position\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2867\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2955\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2950\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2951\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2953\u001b[0m         )\n\u001b[1;32m   2954\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2956\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2957\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2958\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2959\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2960\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2961\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2962\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2963\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2964\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m   2965\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2966\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2967\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2968\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2969\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2970\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2971\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2972\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2973\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[1;32m   2974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2975\u001b[0m     )\n\u001b[1;32m   2976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2978\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2979\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2997\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2998\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3156\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3148\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3149\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3154\u001b[0m )\n\u001b[0;32m-> 3156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   3157\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3158\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   3159\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   3160\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   3161\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   3162\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   3163\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   3164\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   3165\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[1;32m   3166\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   3167\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   3168\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   3169\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   3170\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   3171\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   3172\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   3173\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   3174\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[1;32m   3175\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3176\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py:872\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    868\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m--> 872\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    876\u001b[0m     )\n\u001b[1;32m    878\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ids_or_pair_ids \u001b[38;5;129;01min\u001b[39;00m batch_text_or_text_pairs:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Step 1: Load and Preprocess the Dataset\n",
    "def load_dataset(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found at: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print column names for debugging\n",
    "    print(\"Dataset columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Define expected columns based on provided dataset\n",
    "    expected_columns = ['Chapter', 'Verse', 'Shloka', 'EngMeaning']\n",
    "    \n",
    "    # Check if expected columns exist\n",
    "    if not all(col in df.columns for col in expected_columns):\n",
    "        print(f\"Warning: Expected columns {expected_columns}, but found {df.columns}\")\n",
    "        # Combine available text columns for context\n",
    "        text_columns = [col for col in df.columns if col in ['Shloka', 'EngMeaning', 'HinMeaning', 'WordMeaning']]\n",
    "        if not text_columns:\n",
    "            raise ValueError(\"No suitable text columns found in dataset for context creation.\")\n",
    "        df['context'] = df.apply(lambda row: ' '.join([str(row[col]) for col in text_columns]), axis=1)\n",
    "    else:\n",
    "        # Combine Shloka and EngMeaning for context\n",
    "        df['context'] = df['Shloka'].astype(str) + \" \" + df['EngMeaning'].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Step 2: Format Data for Question-Answering\n",
    "def create_qa_pairs(df):\n",
    "    qa_data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        context = row['context']\n",
    "        \n",
    "        # Use Chapter and Verse for question generation\n",
    "        chapter = row['Chapter']\n",
    "        verse = row['Verse']\n",
    "        questions = [\n",
    "            f\"What does verse {verse} of chapter {chapter} say?\",\n",
    "            f\"What is the teaching in chapter {chapter} verse {verse}?\",\n",
    "            \"What is the meaning of this verse?\"\n",
    "        ]\n",
    "        \n",
    "        for question in questions:\n",
    "            qa_data.append({\n",
    "                'question': question,\n",
    "                'context': context,\n",
    "                'answer': row.get('EngMeaning', context),  # Use EngMeaning as answer, fallback to context\n",
    "                'start_position': 0,  # Simplified; adjust for precise answer span\n",
    "                'end_position': len(row.get('EngMeaning', context))\n",
    "            })\n",
    "    return qa_data\n",
    "\n",
    "# Step 3: Prepare Dataset for Hugging Face\n",
    "def prepare_hf_dataset(qa_data):\n",
    "    dataset = Dataset.from_list(qa_data)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        encodings = tokenizer(\n",
    "            examples['question'],\n",
    "            examples['context'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        encodings['start_positions'] = examples['start_position']\n",
    "        encodings['end_positions'] = examples['end_position']\n",
    "        return encodings\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    return tokenized_dataset, tokenizer\n",
    "\n",
    "# Step 4: Fine-Tune the Model\n",
    "def train_model(tokenized_dataset):\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./gita_qa_model',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    model.save_pretrained('./gita_qa_model')\n",
    "    tokenizer.save_pretrained('./gita_qa_model')\n",
    "    return model, tokenizer\n",
    "\n",
    "# Step 5: Inference Function\n",
    "def answer_question(question, context, model, tokenizer):\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores) + 1\n",
    "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx]\n",
    "    answer = tokenizer.decode(answer_tokens)\n",
    "    return answer\n",
    "\n",
    "# Step 6: Main Function to Run the System\n",
    "def main():\n",
    "    # Path to your dataset\n",
    "    file_path = '/Users/bodapati/Downloads/Bhagwad_Gita.csv'\n",
    "    try:\n",
    "        df = load_dataset(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(\"Dataset loaded successfully. Sample:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    qa_data = create_qa_pairs(df)\n",
    "    tokenized_dataset, tokenizer = prepare_hf_dataset(qa_data)\n",
    "    model, tokenizer = train_model(tokenized_dataset)\n",
    "    \n",
    "    # Interactive QA loop\n",
    "    print(\"\\nBhagavad Gita QA System. Type 'exit' to quit.\")\n",
    "    context = \" \".join(df['context'].tolist())  # Combine all verses for general context\n",
    "    while True:\n",
    "        question = input(\"Ask a question about the Bhagavad Gita: \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        answer = answer_question(question, context, model, tokenizer)\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
